import os
import time
from datetime import date, timedelta
from itertools import product

import pandas as pd

from fastscanner.adapters.candle.parquet import ParquetCandlesProvider
from fastscanner.adapters.candle.partitioned_csv import PartitionedCSVCandlesProvider
from fastscanner.adapters.candle.polygon import PolygonCandlesProvider
from fastscanner.pkg import config
from fastscanner.services.indicators.ports import CandleStore

SYMBOL = "AAPL"
START_DATE = date(2020, 1, 5)
FREQUENCIES = ["1min", "2min", "15min", "1h", "5h", "1d"]

DURATION_MAP = {
    "1D": timedelta(days=1),
    "1M": timedelta(days=30),
    "3M": timedelta(days=90),
}

OUTPUT_DIR = "output"
os.makedirs(OUTPUT_DIR, exist_ok=True)

results = []


def benchmark(provider: CandleStore, label):
    print(f"\n=== {label} Benchmark ===")

    for freq, (duration_label, delta) in product(FREQUENCIES, DURATION_MAP.items()):
        case_name = f"{freq}-{duration_label}"
        start = START_DATE
        end = START_DATE + delta

        print(f"Priming cache for {case_name}...")
        df = provider.get(SYMBOL, start, end, freq)

        n_samples = 100
        elapsed = 0
        for _ in range(n_samples):
            t0 = time.perf_counter()
            df = provider.get(SYMBOL, start, end, freq)
            t1 = time.perf_counter()
            elapsed += t1 - t0

        elapsed = round(elapsed / n_samples, 4)
        row_count = len(df)

        print(f"[{case_name}] {elapsed:.4f}s â€” Rows: {row_count}")
        results.append(
            {
                "Format": label,
                "Range": case_name,
                "Time (s)": elapsed,
                "Rows": row_count,
                "Memory (MB)": round(df.memory_usage(deep=True).sum() / 1024**2, 2),
            }
        )


if __name__ == "__main__":
    polygon = PolygonCandlesProvider(config.POLYGON_BASE_URL, config.POLYGON_API_KEY)
    benchmark(PartitionedCSVCandlesProvider(polygon), "Partitioned CSV")
    # benchmark(
    #     ParquetCandlesProvider(config.POLYGON_BASE_URL, config.POLYGON_API_KEY),
    #     "Parquet",
    # )

    df = pd.DataFrame(results)

    # Create side-by-side comparison
    csv_df = df[df["Format"] == "Partitioned CSV"].set_index("Range")
    # parquet_df = df[df["Format"] == "Parquet"].set_index("Range")

    side_by_side = (
        csv_df[["Rows", "Time (s)", "Memory (MB)"]]
        # .join(
        #     parquet_df[["Rows", "Time (s)", "Memory (MB)"]],
        #     lsuffix=" CSV",
        #     rsuffix=" Parquet",
        # )
        .reset_index()
    )

    side_by_side.columns = [
        "Range",
        "Partitioned CSV Rows",
        "Partitioned CSV Time (s)",
        "Partitioned CSV Memory (MB)",
        # "Parquet Rows",
        # "Parquet Time (s)",
        # "Parquet Memory (MB)",
    ]

    csv_path = os.path.join(OUTPUT_DIR, "benchmark_report.csv")
    side_by_side.to_csv(csv_path, index=False)
    print(f"\nSide-by-side benchmark results saved to: {csv_path}")
